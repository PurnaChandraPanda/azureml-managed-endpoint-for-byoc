{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a single model deployment using a custom container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure parameters, assets, and clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Set endpoint details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "endpoint_name = f\"singlemodfastep-{random.randint(0,10000)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set asset paths\n",
    "Define the directories containing the two model files as well as a directory which contains the scoring script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_path = \"../model-1\"\n",
    "models_path = os.path.join(base_path, \"model\")\n",
    "code_path = os.path.join(base_path, \"onlinescoring\")\n",
    "test_data_path = os.path.join(base_path, \"test-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Examine the models folder\n",
    "The models folder contains two models which will be loaded simultaneously by the scoring script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sklearn_regression_model.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Examine the score script\n",
    "\n",
    "- Score script is located at `code_path/score.py`.\n",
    "- This is just where init() is called in docker initialization. But, run() is a pass here.\n",
    "- As there's other endpoint uri call to be maintained, the fastiapi app based code is hosted on uvicorn.\n",
    "- The fastapi app `code_path/engine/api_engine.py` individual app.get() or app.post(), where all routing points are managed as a destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Examine the Dockerfile\n",
    "The dockerfile is located at `../cli/custom-container/fast-in-dockerfile/minimal-single-model-conda-in-dockerfile.dockerfile`. It uses the AzureML Inference Minimal CPU image as a base and adds relevant dependencies for the scoring script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Create an MLClient instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    CodeConfiguration,\n",
    "    Environment,\n",
    "    ProbeSettings,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "## Get handle of ml workspace\n",
    "### Get the workspace from the config file\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient.from_config(\n",
    "    credential\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the workspace aligned values\n",
    "_resource_group = ml_client.resource_group_name\n",
    "_workspace_name = ml_client.workspace_name\n",
    "_location = ml_client.workspaces.get(ml_client.workspace_name).location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define and create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ManagedOnlineEndpoint(name=endpoint_name)\n",
    "poller = ml_client.online_endpoints.begin_create_or_update(endpoint)\n",
    "poller.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Confirm that creation was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint creation succeeded\n",
      "endpoint.provisioning_state:  Succeeded\n",
      "endpoint.scoring_uri:  https://singlemodfastep-1092.eastus2.inference.ml.azure.com/score\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.exceptions import DeploymentException\n",
    "\n",
    "status = poller.status()\n",
    "if status != \"Succeeded\":\n",
    "    raise DeploymentException(status)\n",
    "else:\n",
    "    print(\"Endpoint creation succeeded\")\n",
    "    endpoint = poller.result()\n",
    "    print(\"endpoint.provisioning_state: \", endpoint.provisioning_state)\n",
    "    print(\"endpoint.scoring_uri: \", endpoint.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'singlemodfastep-1092'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the BYOC image and store on ACR\n",
    "\n",
    "cmd = (\n",
    "    # keep the variable echo, but do not hide stderr\n",
    "    \"source util-scripts/build_byoc_image.sh 1>/dev/null \"\n",
    "    \"&& printf '%s' \\\"$BYOC_IMAGE_NAME_PATH\\\"\"\n",
    ")\n",
    "\n",
    "byoc_generate_process = subprocess.run(\n",
    "    [\n",
    "        \"bash\", \"-c\", cmd, \n",
    "        endpoint_name, _resource_group, _workspace_name, _location\n",
    "    ],\n",
    "    text=True,\n",
    "    capture_output=True,   # get both stdout and stderr\n",
    "    check=False            # we will inspect the return code ourselves\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image path : mlws01contreg.azurecr.io/azureml-examples/minimal-single-model-fast2-in-dockerfile:1\n",
      "Exit code  : 0\n",
      "--------- build_byoc_image.sh stderr ---------\n",
      "\n",
      "**************************************************************************************************************\n",
      "* WARNING:                                                                                                   *\n",
      "* Extension \"azure-cli-ml\" cannot be used along with extension \"ml\". This may result in unexpected behaviour.*\n",
      "* Please remove azure-cli-ml extension by running  \"az extension remove -n azure-cli-ml                      *\n",
      "**************************************************************************************************************\n",
      "                \n",
      "\n",
      "**************************************************************************************************************\n",
      "* WARNING:                                                                                                   *\n",
      "* Extension \"azure-cli-ml\" cannot be used along with extension \"ml\". This may result in unexpected behaviour.*\n",
      "* Please remove azure-cli-ml extension by running  \"az extension remove -n azure-cli-ml                      *\n",
      "**************************************************************************************************************\n",
      "                \n",
      "WARNING: Packing source code into tar to upload...\n",
      "WARNING: Uploading archived source code from '/tmp/build_archive_1fdd84862ba040ffa52ab5688e08f9cb.tar.gz'...\n",
      "WARNING: Sending context (10.046 KiB) to registry: mlws01contreg...\n",
      "WARNING: Queued a build with ID: ch1x\n",
      "WARNING: Waiting for an agent...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "byoc_image_name_path = byoc_generate_process.stdout.strip()\n",
    "\n",
    "print(\"Image path :\", byoc_image_name_path)\n",
    "print(\"Exit code  :\", byoc_generate_process.returncode)\n",
    "if byoc_generate_process.stderr:\n",
    "    print(\"--------- build_byoc_image.sh stderr ---------\")\n",
    "    print(byoc_generate_process.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !./util-scripts/build_byoc_image.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "deployment_name=\"custom-container-singlemodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import OnlineRequestSettings\n",
    "\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=Model(name=\"minimal-singlemodel\", path=models_path),\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=code_path, scoring_script=\"score.py\"\n",
    "    ),\n",
    "    environment=Environment(\n",
    "        name=\"minimal-singlemodel\",\n",
    "        image=byoc_image_name_path,\n",
    "        inference_config={\n",
    "            \"liveness_route\": {\"path\": \"/\", \"port\": 8003},\n",
    "            \"readiness_route\": {\"path\": \"/\", \"port\": 8003},\n",
    "            \"scoring_route\": {\"path\": \"/score\", \"port\": 8003},\n",
    "        },\n",
    "    ),\n",
    "    instance_type=\"Standard_DS3_v2\",\n",
    "    instance_count=1,\n",
    "    liveness_probe=ProbeSettings(\n",
    "        initial_delay=120\n",
    "    ),\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        request_timeout_ms=120000\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint singlemodfastep-1092 exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................................................."
     ]
    }
   ],
   "source": [
    "poller = ml_client.online_deployments.begin_create_or_update(deployment)\n",
    "poller.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Confirm that creation was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment creation succeeded\n",
      "deployment.name:  custom-container-singlemodel\n",
      "deployment.provisioning_state:  Succeeded\n"
     ]
    }
   ],
   "source": [
    "status = poller.status()\n",
    "if status != \"Succeeded\":\n",
    "    raise DeploymentException(status)\n",
    "else:\n",
    "    print(\"Deployment creation succeeded\")\n",
    "    deployment = poller.result()\n",
    "    print(\"deployment.name: \", deployment.name)\n",
    "    print(\"deployment.provisioning_state: \", deployment.provisioning_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Set traffic to 100% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
     ]
    }
   ],
   "source": [
    "endpoint.traffic = {deployment_name: 100}\n",
    "poller = ml_client.begin_create_or_update(endpoint)\n",
    "poller.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the endpoint\n",
    "The `model` JSON field in both JSON payloads indicates which model to score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'your_result'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "res = ml_client.online_endpoints.invoke(\n",
    "    endpoint_name, request_file=os.path.join(test_data_path, \"request.json\")\n",
    ")\n",
    "print(json.loads(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Test the model with all its APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script Output:\n",
      " Getting access key and scoring URL...\n",
      "Base URL is https://singlemodfastep-1092.eastus2.inference.ml.azure.com\n",
      "Scoring URL is https://singlemodfastep-1092.eastus2.inference.ml.azure.com/score\n",
      " \n",
      "https://singlemodfastep-1092.eastus2.inference.ml.azure.com/\n",
      "\"healthy\" \n",
      "https://singlemodfastep-1092.eastus2.inference.ml.azure.com/score1\n",
      "\"{\\\"message\\\": \\\"This is a custom GET endpoint\\\"}\" \n",
      "https://singlemodfastep-1092.eastus2.inference.ml.azure.com/score\n",
      "{\"prediction\":\"your_result\"} \n",
      "https://singlemodfastep-1092.eastus2.inference.ml.azure.com/predict\n",
      "{\"result\":\"your_prediction_result\"} \n",
      "\n",
      "\n",
      "Script Errors:\n",
      " \n",
      "**************************************************************************************************************\n",
      "* WARNING:                                                                                                   *\n",
      "* Extension \"azure-cli-ml\" cannot be used along with extension \"ml\". This may result in unexpected behaviour.*\n",
      "* Please remove azure-cli-ml extension by running  \"az extension remove -n azure-cli-ml                      *\n",
      "**************************************************************************************************************\n",
      "                \n",
      "\n",
      "**************************************************************************************************************\n",
      "* WARNING:                                                                                                   *\n",
      "* Extension \"azure-cli-ml\" cannot be used along with extension \"ml\". This may result in unexpected behaviour.*\n",
      "* Please remove azure-cli-ml extension by running  \"az extension remove -n azure-cli-ml                      *\n",
      "**************************************************************************************************************\n",
      "                \n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100     9  100     9    0     0    225      0 --:--:-- --:--:-- --:--:--   225\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:06 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:07 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:08 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:09 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:10 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:11 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:12 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:13 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:14 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:15 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:16 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:17 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:18 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:19 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:20 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:21 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:22 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:23 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:24 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:25 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:26 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:27 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:28 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:29 --:--:--     0\n",
      "  0    50    0     0    0     0      0      0 --:--:--  0:00:30 --:--:--     0\n",
      "100    50  100    50    0     0      1      0  0:00:50  0:00:30  0:00:20    13\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    81  100    28  100    53    682   1292 --:--:-- --:--:-- --:--:--  1975\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   101  100    35  100    66   1093   2062 --:--:-- --:--:-- --:--:--  3156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the shell script and capture its output\n",
    "result = subprocess.run([\n",
    "    \"bash\", \n",
    "    \"util-scripts/invoke_endpoint.sh\",\n",
    "    endpoint_name, _resource_group, _workspace_name, _location, test_data_path\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "# Print stdout (normal output)\n",
    "print(\"Script Output:\\n\", result.stdout)\n",
    "\n",
    "# Optionally, print stderr (errors, if any)\n",
    "if result.stderr:\n",
    "    print(\"\\nScript Errors:\\n\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Delete assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poller = ml_client.online_endpoints.begin_delete(name=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
